{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "29beee72",
   "metadata": {},
   "source": [
    "# Inference Runbook\n",
    "\n",
    "## Overview\n",
    "This notebook provides a step-by-step guide to configuring and submitting building footprint inference requests. Each step corresponds to a heading in the notebook for easier navigation. The 'kernel' for this notebook appears in upper right, should be 'conda_python3'. \n",
    "If it is not, please change the kernel via Kernel in file menu.\n",
    "\n",
    "### Steps\n",
    "1. **Import Dependencies**\n",
    "   Load the necessary libraries and modules.\n",
    "\n",
    "2. **Configure Inference Job**\n",
    "   Set up the parameters and configurations for the inference job.\n",
    "\n",
    "3. **AOI / Input GeoTIFF Selection**\n",
    "   Choose the Area of Interest (AOI) and the input GeoTIFF file.\n",
    "\n",
    "4. **Start Model Endpoint**\n",
    "   Initialize the model endpoint for inference.\n",
    "\n",
    "5. **Run Inference Job**\n",
    "   Submit the inference request to the model endpoint.\n",
    "\n",
    "6. **Stop Model Endpoint**\n",
    "   Shut down the model endpoint after the job completes.\n",
    "\n",
    "7. **Visualize Results**\n",
    "   Generate visualizations to review the inference outputs.\n",
    "\n",
    "8. **Filter GeoJSON Detections**\n",
    "   Refine the detections by applying filtering criteria.\n",
    "\n",
    "9. **Save Filtered GeoJSON to Shapefile**\n",
    "   Save the filtered GeoJSON data as a Shapefile for further use."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efc91677",
   "metadata": {},
   "source": [
    "# 1. Import Required Dependencies\n",
    "\n",
    "### Notebook Setup and Configuration\n",
    "\n",
    "This section sets up the environment and imports the necessary modules to run the inference pipeline.\n",
    "\n",
    "#### Key Steps:\n",
    "1. **Enable Auto-Reload**: Automatically reloads Python modules when they are modified using `%load_ext autoreload`.\n",
    "2. **Path Configuration**: Adds the `inference` directory to the Python path to ensure all required modules can be imported.\n",
    "3. **Library Imports**: Loads key libraries such as:\n",
    "   - **Boto3** for AWS service interaction.\n",
    "   - **SageMaker SDK** for endpoint management.\n",
    "   - **Custom utilities** from the `utils` module for pipeline-specific functions.\n",
    "4. **Warning and Logging Management**:\n",
    "   - Suppresses unnecessary warnings and configures SageMaker logs to show only critical messages.\n",
    "5. **AWS Environment Initialization**:\n",
    "   - Sets up a SageMaker session and retrieves essential details, including:\n",
    "     - Role\n",
    "     - Region\n",
    "     - Account ID\n",
    "   - Prints these details for confirmation."
   ]
  },
  {
   "cell_type": "code",
   "id": "36a9fb84",
   "metadata": {},
   "source": [
    "# Enable autoreload to automatically reload modules when they are modified\n",
    "%load_ext autoreload\n",
    "    \n",
    "# Add the directory containing our notebook\n",
    "import sys\n",
    "module_directory = \"/home/ec2-user/guidance-for-processing-overhead-imagery-on-aws/documentation/inference\"\n",
    "sys.path.append(module_directory)\n",
    "\n",
    "# Import required libraries\n",
    "from sagemaker import get_execution_role, Session\n",
    "import boto3\n",
    "import warnings\n",
    "import logging\n",
    "import pprint\n",
    "from inference_utils import (\n",
    "    get_input_imagery,\n",
    "    update_modelrunner_tasks,\n",
    "    wait_for_endpoint,\n",
    "    submit_all_images_for_processing,\n",
    "    wait_for_job_completion,\n",
    "    merge_and_save_results_local,\n",
    "    get_geojson_name_for,\n",
    "    plot_inference_results,\n",
    "    filter_geojson_with_threshold,\n",
    "    compare_inference_results,\n",
    "    save_geojson_as_shapefile,\n",
    "    create_sagemaker_endpoint,\n",
    "    delete_sagemaker_endpoint,\n",
    "    save_config_to_yaml\n",
    ")\n",
    "# Initialize PrettyPrinter with specified indentation\n",
    "pp = pprint.PrettyPrinter(indent=3)\n",
    "\n",
    "# Suppress specific warnings\n",
    "warnings.filterwarnings(\n",
    "    \"ignore\", \n",
    "    message=\"numpy.dtype size changed\"\n",
    ")\n",
    "\n",
    "# Set SageMaker logging level to WARNING\n",
    "logging.getLogger(\"sagemaker\").setLevel(logging.WARNING)\n",
    "\n",
    "# Optionally, suppress other specific logs if needed\n",
    "logging.getLogger(\"sagemaker.config\").setLevel(logging.ERROR)\n",
    "\n",
    "# Initialize SageMaker session and retrieve details\n",
    "session = boto3.session.Session()\n",
    "region = session.region_name\n",
    "sm_boto3 = boto3.client(\"sagemaker\")\n",
    "sm_session = Session(boto_session=session)\n",
    "role = get_execution_role()\n",
    "account = sm_session.account_id()\n",
    "\n",
    "# Print essential details (limited to avoid clutter)\n",
    "print(f\"Role: {role}\")\n",
    "print(f\"Region: {region}\")\n",
    "print(f\"Account: {account}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "54027a27",
   "metadata": {},
   "source": [
    "# 2. Configure Inference Job\n",
    "\n",
    "### Preparing the Inference Configuration\n",
    "\n",
    "#### Steps:\n",
    "1. Update the provided `inference_confg` fields in to match your specific requirements.\n",
    "2. Run the cell and it will automatically save a copy of your config locally.\n",
    "\n",
    "#### Important Notes:\n",
    "- If you edit the configuration file while using this notebook, **make sure to rerun the next 'Load' cell** to ensure the notebook uses the latest version of your configuration.  \n",
    "- After running the 'Load' cell, verify that your changes are correctly reflected in the printed configuration."
   ]
  },
  {
   "cell_type": "code",
   "id": "e1560c94-a1f4-47b5-a76b-4aa787e9eb62",
   "metadata": {},
   "source": [
    "# Define the inference configuration inline\n",
    "inference_config = {\n",
    "    \"region\": \"us-east-1\",\n",
    "    \"account\": \"12345678901\",\n",
    "    \"jobName\": \"inference-test\",  # Job name, also used for output prefix/folder.\n",
    "\n",
    "    # Input Imagery Configuration\n",
    "    # Uncomment the line below if input type is \"aoi\" or \"years\":\n",
    "    # \"imagery_meta_s3_uri\": \"s3://test-tile-metadata-bucket_test_tile_metadata\",\n",
    "    \"imagery_bucket_s3\": \"test-imagery-bucket\",\n",
    "    \"which_input_option\": \"s3_path\",  # Options: 'aoi', 'years', 's3_path'.\n",
    "\n",
    "    # Uncomment one of these keys based on the input type:\n",
    "    # \"aoi\": \"s3://nc-building-footprints-app/SurryCounty_aoi.json\",\n",
    "    # \"years\": [\"2022\"],  # Example years.\n",
    "    \"s3_path\": \"s3://osml-test-images-12345678901/small.tif\",\n",
    "\n",
    "    # Model Information\n",
    "    \"model_endpoint_config_name\": \"aircraft-config\",  # Choose your endpoint config.\n",
    "    \"model_endpoint_name\": \"aircraft\",\n",
    "    \"tile_overlap\": 512,  # Tile overlap in pixels.\n",
    "\n",
    "    # Fixed pipeline values\n",
    "    \"modelrunner_cluster_name\": \"MRCluster\",\n",
    "    \"modelrunner_service_name\": \"OSML-ModelRunner-MRDataplaneMRService79C973F6-Nm6ZwKR4tmgL\",\n",
    "    \"modelrunner_task_count\": 1,\n",
    "\n",
    "    # GeoJSON Output Configuration\n",
    "    \"AOI_S3_output_bucket\": \"mr-bucket-sink-12345678901\",\n",
    "    \"AOI_S3_output_prefix\": \"test/\",  # Appends job name automatically.\n",
    "\n",
    "    # Post-Processing Parameters\n",
    "    \"detection_threshold\": 0.3,  # Confidence threshold for detections.\n",
    "}\n",
    "\n",
    "save_config_to_yaml(inference_config, \"/home/ec2-user/SageMaker/inference/inference_config.yml\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "2e1511f9",
   "metadata": {},
   "source": [
    "# 3. Input File Selection\n",
    "\n",
    "### Selecting Files for Inference\n",
    "\n",
    "The pipeline requires a Python list of GeoTIFFs stored in S3, assigned to the variable `all_images`.\n",
    "\n",
    "### Input Options:\n",
    "There are four ways to populate this list:\n",
    "1. **AOI (Area of Interest)**  \n",
    "2. **Years**  \n",
    "3. **S3 Path**  \n",
    "4. **File**\n",
    "\n",
    "### Important Notes:\n",
    "Ensure you specify your desired input method in the configuration file under the `which_input_option` field. If there is a CRS warning from AOI, we can ignore as we're confident the original CRS is indeed EPSG:6543. If there is a CRS warning from AOI, we can ignore as we're confident the original CRS is indeed EPSG:6543"
   ]
  },
  {
   "cell_type": "code",
   "id": "40101dd3-81d7-4b14-8a3e-d9d8077a5a6a",
   "metadata": {},
   "source": [
    "# Retrieve the list of input imagery based on the configuration\n",
    "all_images = get_input_imagery(inference_config)\n",
    "\n",
    "# Verify the number of images retrieved and inspect one of the paths\n",
    "print(f\"Number of images: {len(all_images)}\")\n",
    "if all_images:\n",
    "    print(f\"Example image path: {all_images[0]}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "83697184",
   "metadata": {},
   "source": [
    "# 4. Start Model Endpoint\n",
    "\n",
    "### Starting the AI/ML Model for Inference\n",
    "\n",
    "Before running the inference job, we need to start the AI/ML model endpoint if it is not already running.\n",
    "The **final step** of this process will be shutting the model down to avoid unnecessary costs.\n",
    "\n",
    "### Explore Endpoint Details\n",
    "- If you're curious, you can check the [SageMaker endpoints in the AWS Console](https://us-east-1.console.aws.amazon.com/sagemaker/home?region=us-east-1#/endpoints) before starting the endpoint, and revisit after it has been started.\n",
    "\n",
    "### Endpoint Configurations\n",
    "- The instance type is determined by the `model_endpoint_config` specified in the configuration file.\n",
    "- To create a new model endpoint configuration for hosting your model on SageMaker, you can use the [AWS SageMaker Create Endpoint Configuration Console](https://docs.aws.amazon.com/sagemaker/latest/APIReference/API_CreateEndpointConfig.html).\n",
    "- [AWS On-Demand Instance Pricing](https://aws.amazon.com/ec2/pricing/on-demand/) provides cost details for each instance type.\n",
    "\n",
    "### Recommendation:\n",
    "- Initial benchmarking has been performed using **P3 instances** for optimal performance.\n",
    "- Once the team is familiar with the platform, switching to **G4 instances** is recommended to reduce costs while still achieving usable inference results.\n",
    "  \n",
    "### Inspecting Endpoint Status\n",
    "\n",
    "It typically takes **5–10 minutes** for the endpoint to become fully operational, as SageMaker provisions the ECS instance hosting it.\n",
    "\n",
    "#### Monitoring Options:\n",
    "1. **AWS Console**: You can check the endpoint status in the [SageMaker Endpoints section](https://us-east-1.console.aws.amazon.com/sagemaker/home?region=us-east-1#/endpoints) of the AWS Console.\n",
    "2. **Programmatic Check**: Alternatively, you can use the `DescribeEndpoint` API to programmatically get the status of the endpoint. This is what the loop does at the end of the cell bellow for you."
   ]
  },
  {
   "cell_type": "code",
   "id": "41fc6695",
   "metadata": {},
   "source": [
    "# Create the SageMaker endpoint using the specified configuration\n",
    "create_sagemaker_endpoint(inference_config)\n",
    "\n",
    "# Wait for the endpoint to be ready, fail, or time out\n",
    "wait_for_endpoint(inference_config['model_endpoint_name'], sm_boto3, check_interval=30, timeout_minutes=30)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "5e6920d0",
   "metadata": {},
   "source": [
    "# 5. Start Inference Job\n",
    "\n",
    "### Starting Inference\n",
    "\n",
    "With the model endpoint now **ready**, we can proceed to run the inference process!\n",
    "\n",
    "### Monitoring Job Progress:\n",
    "- The best way to track progress is by reading the **SQS queues** used in the inference pipeline.\n",
    "- Keep in mind that some images might **fail**, so the counts for \"total sent\" and \"total processed\" may not match exactly.\n",
    "\n",
    "### Job Completion:\n",
    "- The job is considered **complete** when the **'total images processed'** count does not change over a period of **5 minutes**.\n",
    "- Run the next cell, and the rest below, **only when the job is complete**.\n",
    "- The `wait_for_job_completion` function helps track the progress of the inference job by monitoring the number of processed GeoJSON files in the specified S3 bucket.\n",
    "\n",
    "#### How It Works:\n",
    "1. **Checks GeoJSON Results**: The function compares the number of GeoJSON files in the output S3 path to the total number of input images.\n",
    "2. **Completion Criteria**: The job is considered complete when the number of processed GeoJSON files equals or exceeds the total number of input images.\n",
    "3. **Timeout Handling**: \n",
    "   - The function waits for updates in a loop, checking the progress at regular intervals (`check_interval`).\n",
    "   - If no updates are detected within the timeout period (`timeout_minutes`), the function exits, indicating the job did not complete in time."
   ]
  },
  {
   "cell_type": "code",
   "id": "59e9fbd8-55fe-49ca-9192-317325636e25",
   "metadata": {},
   "source": [
    "# Spin up some tasks for ModelRunner\n",
    "update_modelrunner_tasks(inference_config, 3)\n",
    "\n",
    "# Submit all selected images for inference processing\n",
    "submit_all_images_for_processing(inference_config, all_images[:])\n",
    "\n",
    "# Wait for the job to complete and report the status\n",
    "wait_for_job_completion(inference_config, all_images)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "9a7edfc6-6283-45f3-90d6-1b12e251d305",
   "metadata": {},
   "source": [
    "# 6. Stop Model Endpoint\n",
    "\n",
    "### Important: Cost of the Model Endpoint\n",
    "\n",
    "The model endpoint is the **most expensive component** of the pipeline. **Do not leave it running** when not in use!\n",
    "\n",
    "#### Cost Breakdown:\n",
    "- **G4dn.2xlarge**: ~$0.75/hour\n",
    "- **P3.2xlarge**: ~$3.00/hour  \n",
    "[View AWS EC2 On-Demand Pricing](https://aws.amazon.com/ec2/pricing/on-demand/)\n",
    "\n",
    "**Note:** All benchmarking has been conducted using **P3 instances** for optimal performance."
   ]
  },
  {
   "cell_type": "code",
   "id": "046fd1c3-a1f9-4529-b20b-d26c326d1341",
   "metadata": {},
   "source": [
    "# Reset the number of model runner tasks to zero\n",
    "update_modelrunner_tasks(inference_config, 0)\n",
    "print(\"✅ Model runner tasks have been reset to zero.\")\n",
    "\n",
    "# Attempt to delete the SageMaker endpoint\n",
    "delete_sagemaker_endpoint(inference_config['model_endpoint_name'], sm_session)\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "e10a64b2-dc23-4518-b8ef-eafbdd33b1eb",
   "metadata": {},
   "source": [
    "# 7. Post Processing\n",
    "\n",
    "### Download and Explore Your Results\n",
    "\n",
    "After running the inference pipeline, you can:\n",
    "- **Download the Shapefile or GeoJSON**: Use these outputs with your favorite workflow tools, such as **ArcGIS**, **QGIS**, or [**Kepler.gl**](https://kepler.gl/).\n",
    "- **Preview the Results in the Notebook**: Visualize individual images with their corresponding GeoJSON footprints to ensure the results meet your expectations."
   ]
  },
  {
   "cell_type": "code",
   "id": "9f7ed897-93e4-4c03-b122-de5d839ea0a6",
   "metadata": {},
   "source": [
    "# Downloading Your Results\n",
    "\n",
    "# Each 10k tile has a geojson result that needs to be merged into a single geojson.\n",
    "merge_and_save_results_local(inference_config)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "01b3b22a-b5fd-4159-9484-0836582f49ac",
   "metadata": {},
   "source": [
    "### Perform Post-Processing\n",
    "\n",
    "If the resulting features require further refinement (e.g., correcting rounded corners), refer to the **`doc/orthogonalization`** guide for examples of post-processing techniques."
   ]
  },
  {
   "cell_type": "code",
   "id": "66d6d10f-7f99-4b85-8821-a73c3ace8218",
   "metadata": {},
   "source": [
    "# Visualizing Results for a Single Image\n",
    "\n",
    "# Note: Visualizing the entire AOI is not practical in this notebook.\n",
    "# Instead, we can view any of the individual images (e.g., one of the 10k images),\n",
    "# overlay with the resulting GeoJSON footprints.\n",
    "# The output S3 bucket contains GeoJSON results with the same filenames as the images.\n",
    "\n",
    "# Specify the index of the image to view\n",
    "image_to_view = 0  # Change this index to visualize a different image\n",
    "\n",
    "# Select the image and corresponding GeoJSON\n",
    "image = all_images[image_to_view]\n",
    "geojson = get_geojson_name_for(inference_config, image)\n",
    "\n",
    "# Set the detection threshold\n",
    "# Option 1: Use the threshold from the configuration file\n",
    "thresh = inference_config['detection_threshold']\n",
    "\n",
    "# Option 2: Override the threshold value here\n",
    "# Uncomment the next line to override\n",
    "# thresh = 0.3\n",
    "\n",
    "# Display the threshold being used\n",
    "print(f\"Using detection threshold: {thresh}\")\n",
    "\n",
    "# Plot the image with the GeoJSON footprints (this may take a few moments)\n",
    "plot_inference_results(image, geojson, thresh)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "e4d86689-ee1d-4f56-8f1f-78e373521fb3",
   "metadata": {},
   "source": [
    "## Comparing Inference Results\n",
    "\n",
    "The `compare_inference_results` function allows you to visually compare the outputs of two models for a given input image. This is particularly useful for evaluating improvements or differences between models.\n",
    "\n",
    "### Usage Example:\n",
    "To compare results, call the function with:\n",
    "1. The input image file path (local or S3).\n",
    "2. The S3 URI for the first model's GeoJSON output.\n",
    "3. The S3 URI for the second model's GeoJSON output."
   ]
  },
  {
   "cell_type": "code",
   "id": "96e7be8e-a3c4-45d7-ba12-2333adb75767",
   "metadata": {},
   "source": [
    "compare_inference_results(\n",
    "    \"s3://osml-test-images-12345678901/small.tif\",\n",
    "    \"s3://mr-bucket-sink-12345678901/new_model_job_id/new_model_output.geojson\",\n",
    "    \"s3://mr-bucket-sink-12345678901/old_model_job_id/old_model_output.geojson\"\n",
    ")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "729df87b-a354-48b9-89f7-11431521daa2",
   "metadata": {},
   "source": [
    "# 8. Filter and Save Detections\n",
    "\n",
    "### Save Filtered Results Using a Threshold\n",
    "\n",
    "Once you have identified an appropriate detection threshold, you can apply it to filter the results. This process involves:\n",
    "\n",
    "1. **Filtering Features**: The threshold is used to retain only the features that meet your confidence level, ensuring the output focuses on the most reliable detections.\n",
    "2. **Saving Results**:\n",
    "   - A filtered **GeoJSON file** is created, containing only the features that pass the threshold.\n",
    "   - The GeoJSON can also be saved as a **Shapefile** for compatibility with a wider range of tools.\n",
    "\n",
    "These filtered outputs can be used for further analysis, visualization, or integration into workflows with tools like ArcGIS, QGIS, or Kepler.gl."
   ]
  },
  {
   "cell_type": "code",
   "id": "d4b50e01-b10b-4611-b7c2-79902235a81a",
   "metadata": {},
   "source": [
    "# Set the detection threshold\n",
    "# Option 1: Use the threshold from the configuration file\n",
    "thresh = inference_config['detection_threshold']\n",
    "\n",
    "# Option 2: Override the threshold value here\n",
    "# Uncomment the next line to override\n",
    "# thresh = 0.3\n",
    "\n",
    "# Display the threshold being used\n",
    "print(f\"Using detection threshold: {thresh}\")\n",
    "\n",
    "# Step 1: Save a new GeoJSON file with the filtered results\n",
    "thresholded_results_filename = filter_geojson_with_threshold(inference_config, thresh)\n",
    "\n",
    "# Step 2: Convert and save the filtered GeoJSON as a Shapefile\n",
    "save_geojson_as_shapefile(inference_config, thresh, thresholded_results_filename)"
   ],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
